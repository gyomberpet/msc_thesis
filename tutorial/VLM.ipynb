{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/navsim/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/anaconda3/envs/navsim/lib/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/anaconda3/envs/navsim/lib/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:16<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image shows a city street intersection with several key elements related to driving:\\n\\n1. **Traffic Lights**: There are traffic lights at the intersection, with one visible in the center of the image. The light is currently red, indicating that vehicles must stop.\\n\\n2. **Pedestrian Crosswalk**: The intersection has a pedestrian crosswalk marked with white lines, indicating where pedestrians can cross the street safely.\\n\\n3. **Vehicles**: There is a black SUV parked on the left side of the image, and a few other vehicles are visible in the distance, including a white car and a black car.\\n\\n4. **Buildings**: The buildings on both sides of the street are multi-story and appear to be commercial or residential structures. The building on the left has large windows and a sign that is partially visible.\\n\\n5. **Street Signs**: There is a street sign visible in the center of the image, indicating the name of the street. The sign is partially obscured by the traffic light.\\n\\n6. **Pedestrians**: There are a few pedestrians visible near the crosswalk, indicating that the area is active and populated.\\n\\n7. **Sidewalks**: The sidewalks are visible on both sides of the street, with some trees and greenery along the right side.\\n\\n8. **Fire Hydrant**: There is a fire hydrant visible on the right side of the image, near the sidewalk.\\n\\n9. **Parking**: There is a parking area on the right side of the image, with a few parking spaces visible.\\n\\n10. **Street Lamps**: Street lamps are visible along the sidewalk on the right side of the image.\\n\\nOverall, the image depicts a typical city street intersection with traffic lights, crosswalks, and buildings, and it is important for drivers to be aware of the traffic signals and pedestrian activity to navigate safely.']\n"
     ]
    }
   ],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "gpu_id=0\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=torch.float16) # , torch_dtype=torch.float16\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# instruction = \"\"\"You are an Autonomous Driving AI assistant. You receive a 1024*256 pixels image of the front view, from the driver's perspective.\n",
    "# Your task is to analyze the images and provide insights based on the visual data. Focus on the followings:\n",
    "# - traffic participants (vehicles, pedestrians etc.)\n",
    "# - traffic lights with color\n",
    "# As your result please use the following template:\n",
    "# {{\n",
    "#     traffic_participants: [\n",
    "#         participant_1_name: {{\n",
    "#             position: x1,y1 - x2,y2 \n",
    "#         }},\n",
    "#         participant_2_name: {{\n",
    "#             position: x1,y1 - x2,y2 \n",
    "#         }},\n",
    "#         ...\n",
    "#     ],\n",
    "#     traffic_lights: [\n",
    "#         light_1: {{\n",
    "#             position: x1,y1 - x2,y2,\n",
    "#             color:\n",
    "#         }},\n",
    "#         light_2: {{\n",
    "#             position: x1,y1 - x2,y2,\n",
    "#             color:\n",
    "#         }}\n",
    "#         ...\n",
    "#     ]\n",
    "# }}\"\"\"\n",
    "\n",
    "instruction = \"\"\"You receive an image from the driver's perspective. Your task is to describe the image in high details,\n",
    "so the driver can make informed driving decisions. Only focus on objects related to driving, like vehicles, pedestrian traffic lights etc.\"\"\"\n",
    "#- any important objects that can affect to make inform driving decisions\"\"\"\n",
    "messages = [\n",
    "    { \n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are an Autonomous Driving AI assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"/root/workdir/NAVSIM/navsim/tutorial/front_view_2.jpg\",\n",
    "                \"resized_height\": 256,\n",
    "                \"resized_width\": 1024,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": f\"{instruction}\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "with autocast():\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -0.15,  0.5,  1.57], [  0.2,  0.55,  0.52], [  0.35,  0.45,  0.05], [  0.1,  0.75,  2.35], [  0.5,  0.2,  0.65], [  0.25,  0.3,  1.45], [  0.7,  0.15,  0.4], [  0.2,  0.25,  0.25], [  0.4,  0.85,  1.85], [  0.4,  0.25,  0.45]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "l = 10\n",
    "example = [[random.uniform(0, 1) for _ in range(3)] for _ in range(10)]\n",
    "prompt = f\"\"\"Your task is to predict the optimal future waypoints of the vehicle, that you are driving, based on the description of current traffic conditions.\n",
    "Here is the description: {output_text}\n",
    "Your response should only be a list with exaclty {l} items, where each item is a 3 length long list, representing a single waypoint of float numbers (x, y, heading).\n",
    "Example format of response: {example}\n",
    "You have to return a {l}*3 flaot list even if you are not sure about the exact numbers. Please write the numbers, that you think alligns the best with the description.\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an Autonomous Driving AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.4616067224992072, 0.573124888842575, 0.9154594068375469],\n",
       " [0.05076615814367358, 0.3129030140499487, 0.24719893777321644],\n",
       " [0.44775105109613245, 0.4262400593326674, 0.043178324234017684],\n",
       " [0.32509374898888066, 0.7139883520707607, 0.32776340405167925],\n",
       " [0.5171830763702864, 0.2185651650523609, 0.6094962197543433],\n",
       " [0.2815316519949732, 0.3850637283156816, 0.8781078240564743],\n",
       " [0.7015948343868881, 0.1798622772687748, 0.3996371806942772],\n",
       " [0.24650230825816344, 0.2760869516899871, 0.23709706586921275],\n",
       " [0.408464720478357, 0.8613100052209202, 0.4145548431967745],\n",
       " [0.42428768955149687, 0.24588191945836724, 0.4588197836103255]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15,  0.5 ,  1.57],\n",
       "       [ 0.2 ,  0.55,  0.52],\n",
       "       [ 0.35,  0.45,  0.05],\n",
       "       [ 0.1 ,  0.75,  2.35],\n",
       "       [ 0.5 ,  0.2 ,  0.65],\n",
       "       [ 0.25,  0.3 ,  1.45],\n",
       "       [ 0.7 ,  0.15,  0.4 ],\n",
       "       [ 0.2 ,  0.25,  0.25],\n",
       "       [ 0.4 ,  0.85,  1.85],\n",
       "       [ 0.4 ,  0.25,  0.45]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "data_list = ast.literal_eval(response)\n",
    "poses = np.array(data_list)\n",
    "poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/navsim/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "navsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
